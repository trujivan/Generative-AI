{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, AutoPeftModelForCausalLM, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tokenize_batch(tokenizer, batch, text_key='verse_text'):\n",
    "    return tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Load an appropriate dataset\n",
    "dataset_name = \"poem_sentiment\"  # Replaced with the actual dataset name\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Assuming that the dataset has a 'train' key, update this if needed\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Check the structure of your dataset and find the correct key for the text\n",
    "print(\"Dataset Structure:\", train_dataset.features)\n",
    "\n",
    "# Use the correct key for the text in your dataset\n",
    "text_key = 'verse_text'  # Replace with the actual key for the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load the Lora model\n",
    "pretrained_model_name = \"facebook/opt-350m\"  # You can choose another model suitable for your task\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    base_model_name_or_path=pretrained_model_name\n",
    ")\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Use get_peft_model to apply LoRA\n",
    "lora_model = get_peft_model(pretrained_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable Parameters Before Fine-Tuning:\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Tokenize and preprocess the datasets for training\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key)\n",
    "test_inputs = tokenize_batch(tokenizer, test_dataset, text_key)\n",
    "\n",
    "# Fine-tune the LoRA model\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_inputs,\n",
    "    eval_dataset=test_inputs,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned LoRA model\n",
    "lora_model.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the fine-tuned LoRA model for inference\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Print trainable parameters after fine-tuning\n",
    "print(\"\\nTrainable Parameters After Fine-Tuning:\")\n",
    "fine_tuned_model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize and evaluate with the fine-tuned model\n",
    "input_text = test_dataset[0][text_key]  # Use any text example from your test dataset\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs_fine_tuned = fine_tuned_model(**inputs)\n",
    "\n",
    "# Perform any necessary evaluation based on your task\n",
    "print(\"Fine-Tuned Model Output:\", outputs_fine_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, AutoPeftModelForCausalLM, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tokenize_batch(tokenizer, batch, id_key='id', text_key='verse_text', label_key='label'):\n",
    "    inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs['labels'] = batch[label_key]\n",
    "    return inputs\n",
    "\n",
    "# Load an appropriate dataset\n",
    "dataset_name = \"poem_sentiment\"  # Replaced with the actual dataset name\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Assuming that the dataset has a 'train' key, update this if needed\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Check the structure of your dataset and find the correct key for the text\n",
    "print(\"Dataset Structure:\", train_dataset.features)\n",
    "\n",
    "# Use the correct key for the text in your dataset\n",
    "text_key = 'verse_text'  # Replace with the actual key for the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load the Lora model\n",
    "pretrained_model_name = \"facebook/opt-350m\"  # You can choose another model suitable for your task\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    base_model_name_or_path=pretrained_model_name\n",
    ")\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Use get_peft_model to apply LoRA\n",
    "lora_model = get_peft_model(pretrained_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable Parameters Before Fine-Tuning:\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Tokenize and preprocess the datasets for training\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key, 'label')\n",
    "test_inputs = tokenize_batch(tokenizer, test_dataset, text_key, 'label')\n",
    "\n",
    "# Fine-tune the LoRA model\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_inputs,\n",
    "    eval_dataset=test_inputs,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned LoRA model\n",
    "lora_model.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the fine-tuned LoRA model for inference\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Print trainable parameters after fine-tuning\n",
    "print(\"\\nTrainable Parameters After Fine-Tuning:\")\n",
    "fine_tuned_model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize and evaluate with the fine-tuned model\n",
    "input_text = test_dataset[0][text_key]  # Use any text example from your test dataset\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs_fine_tuned = fine_tuned_model(**inputs)\n",
    "\n",
    "# Perform any necessary evaluation based on your task\n",
    "print(\"Fine-Tuned Model Output:\", outputs_fine_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54e196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, AutoPeftModelForCausalLM, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tokenize_batch(tokenizer, batch, input_ids='idx', text_key='verse_text', label_key='label'):\n",
    "    inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs['labels'] = batch[label_key]\n",
    "    return inputs\n",
    "\n",
    "# Load an appropriate dataset\n",
    "dataset_name = \"poem_sentiment\"  # Replaced with the actual dataset name\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Add this before creating the Trainer\n",
    "for batch in train_inputs:\n",
    "    print(batch)\n",
    "\n",
    "\n",
    "# Assuming that the dataset has a 'train' key, update this if needed\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Check the structure of your dataset and find the correct key for the text\n",
    "print(\"Dataset Structure:\", train_dataset.features)\n",
    "\n",
    "# Use the correct key for the text in your dataset\n",
    "text_key = 'verse_text'  # Replace with the actual key for the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key=text_key)  # Specify text_key here\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load the Lora model\n",
    "pretrained_model_name = \"facebook/opt-350m\"  # You can choose another model suitable for your task\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    base_model_name_or_path=pretrained_model_name\n",
    ")\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Use get_peft_model to apply LoRA\n",
    "lora_model = get_peft_model(pretrained_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable Parameters Before Fine-Tuning:\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Tokenize and preprocess the datasets for training\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, input_ids='idx', text_key=text_key, label_key='label')  # Specify text_key and label_key here\n",
    "test_inputs = tokenize_batch(tokenizer, test_dataset, input_ids='idx', text_key=text_key, label_key='label')  # Specify text_key and label_key here\n",
    "\n",
    "# Fine-tune the LoRA model\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_inputs,\n",
    "    eval_dataset=test_inputs,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned LoRA model\n",
    "lora_model.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the fine-tuned LoRA model for inference\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Print trainable parameters after fine-tuning\n",
    "print(\"\\nTrainable Parameters After Fine-Tuning:\")\n",
    "fine_tuned_model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize and evaluate with the fine-tuned model\n",
    "input_text = test_dataset[0][text_key]  # Use any text example from your test dataset\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs_fine_tuned = fine_tuned_model(**inputs)\n",
    "\n",
    "# Perform any necessary evaluation based on your task\n",
    "print(\"Fine-Tuned Model Output:\", outputs_fine_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013aaafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, AutoPeftModelForCausalLM, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tokenize_batch(tokenizer, batch, text_key='text', label_key='label'):\n",
    "    inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs['labels'] = batch[label_key]\n",
    "    return inputs\n",
    "\n",
    "# Load an appropriate dataset\n",
    "dataset = load_dataset(\"ethos\", \"binary\")\n",
    "\n",
    "# Add this before creating the Trainer\n",
    "for batch in train_inputs:\n",
    "    print(batch)\n",
    "\n",
    "\n",
    "# Assuming that the dataset has a 'train' key, update this if needed\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Check the structure of your dataset and find the correct key for the text\n",
    "print(\"Dataset Structure:\", train_dataset.features)\n",
    "\n",
    "# Use the correct key for the text in your dataset\n",
    "text_key = 'text'  # Replace with the actual key for the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key=text_key)  # Specify text_key here\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load the Lora model\n",
    "pretrained_model_name = \"facebook/opt-350m\"  # You can choose another model suitable for your task\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    base_model_name_or_path=pretrained_model_name\n",
    ")\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Use get_peft_model to apply LoRA\n",
    "lora_model = get_peft_model(pretrained_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable Parameters Before Fine-Tuning:\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Tokenize and preprocess the datasets for training\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key=text_key, label_key='label')  # Specify text_key and label_key here\n",
    "test_inputs = tokenize_batch(tokenizer, test_dataset, text_key=text_key, label_key='label')  # Specify text_key and label_key here\n",
    "\n",
    "# Fine-tune the LoRA model\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_inputs,\n",
    "    eval_dataset=test_inputs,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned LoRA model\n",
    "lora_model.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the fine-tuned LoRA model for inference\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Print trainable parameters after fine-tuning\n",
    "print(\"\\nTrainable Parameters After Fine-Tuning:\")\n",
    "fine_tuned_model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize and evaluate with the fine-tuned model\n",
    "input_text = test_dataset[0][text_key]  # Use any text example from your test dataset\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs_fine_tuned = fine_tuned_model(**inputs)\n",
    "\n",
    "# Perform any necessary evaluation based on your task\n",
    "print(\"Fine-Tuned Model Output:\", outputs_fine_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcafe132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, AutoPeftModelForCausalLM, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the splits and the percentage of data to be used\n",
    "splits = ['train', 'test']\n",
    "sizes_percent = {'train': 0.1, 'test': 0.1}  # Using only a fraction of the data\n",
    "\n",
    "# Load the poem_sentiment full dataset for specified splits\n",
    "poem_sentiment = {split: load_dataset('poem_sentiment', split=split, trust_remote_code=True) for split in splits}\n",
    "\n",
    "# Thin out the dataset to reduce computational resources\n",
    "for split in splits:\n",
    "    # Shuffle and select a fraction of the data\n",
    "    sampled_size = int(poem_sentiment[split].shape[0] * sizes_percent[split])\n",
    "    poem_sentiment[split] = poem_sentiment[split].shuffle(seed=42).select(range(sampled_size))\n",
    "    \n",
    "def tokenize_batch(tokenizer, batch, id_key='id', text_key='verse_text', label_key='label'):\n",
    "    inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs['labels'] = batch[label_key]\n",
    "    return inputs\n",
    "\n",
    "# Assuming that the dataset has a 'train' key, update this if needed\n",
    "train_dataset = poem_sentiment['train']\n",
    "\n",
    "# Check the structure of your dataset and find the correct key for the text\n",
    "print(\"Dataset Structure:\", train_dataset.features)\n",
    "\n",
    "# Use the correct key for the text in your dataset\n",
    "text_key = 'text'  # Replace with the actual key for the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key=text_key)  # Specify text_key here\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load the Lora model\n",
    "pretrained_model_name = \"facebook/opt-350m\"  # comparing to facebook word model\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    base_model_name_or_path=pretrained_model_name\n",
    ")\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Use get_peft_model to apply LoRA\n",
    "lora_model = get_peft_model(pretrained_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable Parameters Before Fine-Tuning:\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Tokenize and preprocess the datasets for training\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key=text_key, label_key='label')  # Specify text_key and label_key here\n",
    "test_inputs = tokenize_batch(tokenizer, test_dataset, text_key=text_key, label_key='label')  # Specify text_key and label_key here\n",
    "\n",
    "# Fine-tune the LoRA model\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_inputs,\n",
    "    eval_dataset=test_inputs,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned LoRA model\n",
    "lora_model.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the trained Lora model for further evaluation\n",
    "lora_reloaded = AutoPeftModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Print trainable parameters after fine-tuning\n",
    "print(\"\\nTrainable Parameters After Fine-Tuning:\")\n",
    "lora_reloaded.print_trainable_parameters()\n",
    "\n",
    "# Tokenize and evaluate with the fine-tuned model\n",
    "input_text = test_dataset[0][text_key]  # Use any text example from your test dataset\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs_fine_tuned = lora_reloaded(**inputs)\n",
    "\n",
    "# Perform any necessary evaluation based on your task\n",
    "print(\"Fine-Tuned Model Output:\", outputs_fine_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcec421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure: {'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None)}\n",
      "Trainable Parameters Before Fine-Tuning:\n",
      "trainable params: 1,572,864 || all params: 332,769,280 || trainable%: 0.472659014678278\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_ids,attention_mask,head_mask,past_key_values,inputs_embeds,labels,use_cache,output_attentions,output_hidden_states,return_dict,label,labels,label_ids.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Fine-tune the LoRA model\u001b[39;00m\n\u001b[1;32m     70\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     71\u001b[0m     model\u001b[38;5;241m=\u001b[39mlora_model,\n\u001b[1;32m     72\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     73\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_inputs,\n\u001b[1;32m     74\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_inputs,\n\u001b[1;32m     75\u001b[0m )\n\u001b[0;32m---> 76\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned LoRA model\u001b[39;00m\n\u001b[1;32m     79\u001b[0m lora_model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2765\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;124;03mPerform a training step on a batch of inputs.\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2762\u001b[0m \u001b[38;5;124;03m    `torch.Tensor`: The tensor with training loss on this batch.\u001b[39;00m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2764\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m-> 2765\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(inputs)\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2768\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2719\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2717\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs)\n\u001b[1;32m   2718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch received was empty, your model won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be able to train on it. Double-check that your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2721\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m     )\n\u001b[1;32m   2723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2724\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmems\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past\n",
      "\u001b[0;31mValueError\u001b[0m: The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_ids,attention_mask,head_mask,past_key_values,inputs_embeds,labels,use_cache,output_attentions,output_hidden_states,return_dict,label,labels,label_ids."
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, AutoPeftModelForCausalLM, LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tokenize_batch(tokenizer, batch, text_key='verse_text', label_key='label'):\n",
    "    inputs = tokenizer(batch[text_key], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs['labels'] = batch[label_key]\n",
    "    return inputs\n",
    "\n",
    "# Load an appropriate dataset\n",
    "dataset_name = \"poem_sentiment\"  # Replaced with the actual dataset name\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Assuming that the dataset has a 'train' key, update this if needed\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Check the structure of your dataset and find the correct key for the text\n",
    "print(\"Dataset Structure:\", train_dataset.features)\n",
    "\n",
    "# Use the correct key for the text in your dataset\n",
    "text_key = 'verse_text'  # Replace with the actual key for the text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key=text_key)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# Tokenize and preprocess the datasets for training\n",
    "train_inputs = tokenize_batch(tokenizer, train_dataset, text_key=text_key, label_key='label')  # Specify text_key and label_key here\n",
    "test_inputs = tokenize_batch(tokenizer, test_dataset, text_key=text_key, label_key='label')  # Specify text_key and label_key here\n",
    "\n",
    "# Load the Lora model\n",
    "pretrained_model_name = \"facebook/opt-350m\"  # You can choose another model suitable for your task\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    base_model_name_or_path=pretrained_model_name\n",
    ")\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Use get_peft_model to apply LoRA\n",
    "lora_model = get_peft_model(pretrained_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable Parameters Before Fine-Tuning:\")\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Fine-tune the LoRA model\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_inputs,\n",
    "    eval_dataset=test_inputs,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned LoRA model\n",
    "lora_model.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the fine-tuned LoRA model for inference\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Print trainable parameters after fine-tuning\n",
    "print(\"\\nTrainable Parameters After Fine-Tuning:\")\n",
    "fine_tuned_model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize and evaluate with the fine-tuned model\n",
    "input_text = test_dataset[0][text_key]  # Use any text example from your test dataset\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs_fine_tuned = fine_tuned_model(**inputs)\n",
    "\n",
    "# Perform any necessary evaluation based on your task\n",
    "print(\"Fine-Tuned Model Output:\", outputs_fine_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9ded7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
